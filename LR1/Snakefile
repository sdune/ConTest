# Configfile
configfile: "config.yaml"

# Variables
ACC,  = glob_wildcards(config["read_path"]+"{accall}.fastq.gz")
GENOMENONSELF, = glob_wildcards(config["taxdb_path"]+config["nonself"]+"/{genomenonself}.gz")
GENOMESELF, = glob_wildcards(config["taxdb_path"]+config["self"]+"/{genomeself}.gz")

# Exclude test-genome from self database 
ind = GENOMESELF.index(config["test_genome"])              
del GENOMESELF[ind]

GENOME = GENOMENONSELF+GENOMESELF
TAX = [config["nonself"]] * len(GENOMENONSELF) + [config["self"]] * len(GENOMESELF)

# Wildcard constraints
wildcard_constraints:
    id="\d",
    tax="\s+",
    taxon="[^_]+"

# Target rule
rule all:
    input:
        expand("taxonomy_species.{te}.txt",te=config["TE"])
        #expand("LR_{te}/{acc}.reciprocal.fasta", acc=ACC, te=config["TE"])

# Build diamond database
rule build_dmnd:
    input:
        TE=lambda wildcards: config["TE"][wildcards.te]
    output:
        "{te}.dmnd"       
    params:
        TE="{te}"
    shell:
        """
        diamond makedb --in {input.TE} -d {params.TE}
        """

# Identification of TE in reads 
rule diamond:
    input:
        db = "{te}.dmnd",
        fastq = config["read_path"]+"{acc}.fastq.gz"
    output:
        dmnd = "LR_{te}/{acc}.out6"
    log:
        "logs/diamond_{te}/{acc}.log"
    threads:
        56
    shell:  
        """
        diamond blastx --threads {threads} --db {input.db} --out {output.dmnd} --outfmt 6 --query {input.fastq} --more-sensitive 2>{log}
        """

# Get names of reads with TE
rule get_readids:
    input:
        "LR_{te}/{acc}.out6"
    output:
        "LR_{te}/{acc}_filtered.txt"
    shell:
        """
        set +oue
        awk -F'\t' '$11<1e-10 {{print $1}}' {input} > {output} 
        set -oue
        """

# Make fasta file with reads containing TE
rule get_reads:
    input:
        reads="LR_{te}/{acc}_filtered.txt",
        fastq=config["read_path"]+"{acc}.fastq.gz"
    output:
        "LR_{te}/{acc}.dmndhits.fasta"
    shell:
        "seqtk subseq {input.fastq} {input.reads} | seqtk seq -a - > {output} "


# Mask reads
rule RepeatMasker:
    input:
        "LR_{te}/{acc}.dmndhits.fasta"
    output:
        "LR_{te}/{acc}.dmndhits.fasta.out"
    threads:
        56
    params:
        outdir="LR_{te}",
        repbase=config["TEdb"]
    log:
        "logs/RepeatMasker_{te}/{acc}.log"
    shell:
        """
        set +eou
        RepeatMasker -pa {threads} -s -no_is -nocut -nolow -lib {params.repbase} -dir {params.outdir} {input} 
        set -eou
        """


# Select reads that were masked as TE 
rule get_reciprocal_reads:
    input: "LR_{te}/{acc}.dmndhits.fasta.out"
    output: "LR_{te}/{acc}.reciprocal.txt"
    params:
        allTEs=config["recip_TEs"],
        tefasta=lambda wildcards: config["TE"][wildcards.te]
    shell:
        """
        set +eou
        sort -k5,5 -k1,1gr {input} | awk '{{print $0,"\t",$5}}' | uniq -f 15 | 
        grep -f <(grep '>' {params.allTEs} | cut -f 1 | sed 's/>//g') | awk '{{print $5}}'  > {output}
        set -euo
        """

# Make fasta file with reads
rule get_reciprocal_fastas:
    input:
        txt="LR_{te}/{acc}.reciprocal.txt",
        fasta="LR_{te}/{acc}.dmndhits.fasta.masked"
    output:
        "LR_{te}/{acc}.reciprocal.fasta"
    params:
        out="{acc}"
    shell:
        """
        seqtk subseq {input.fasta} {input.txt} > {output}
        """

# Blast reads against genome databases
rule blast_vs_taxOne:
    input:
        reads="LR_{te}/{acc}.reciprocal.fasta",
        db="/home/sdunemann/db/genomes/{taxon}/{genome}.gz"
    output:
        "LR_{te}/{taxon}_{genome}_{acc}.out6"
    threads:
        56
    params:
        dbsize=config["dbsize"]
    shell:
        "blastn -db {input.db} -dbsize {params.dbsize} -query {input.reads} -evalue 1e-03 -outfmt 6 -out {output} -num_threads {threads};"


# Get results
rule taxonomy:
    input:
        expand(expand("LR_{{te}}/{taxon}_{genome}_{{acc}}.out6",zip,taxon=TAX,genome=GENOME), te=config["TE"], acc=ACC),
    output:
        hits="taxonomy_besthits.{te}.txt",
        taxresults="taxonomy_taxa.{te}.txt",
        specresults="taxonomy_species.{te}.txt"
    params:
        te="{te}"
    shell:
        """
        set +eou;
        for i in `find LR_{params.te}/*.out6 -not -empty` ; do
            cat $i >>temp.txt;
        done ;
        sort -k16,16 -k12,12gr temp.txt | uniq -f 15 > {output.hits} ;
        cut -f 13 {output.hits} | sort | uniq -c > {output.taxresults};
        cut -f 14 {output.hits} | sort | uniq -c > {output.specresults};
        rm temp.txt;
        set -eou;
        """

rule report:
    params:
       TE=lambda wildcards: config["TE"][wildcards.te]
    input:
        results="taxonomy_results_{params.TE}.txt",
        stats="taxonomy_stats_{params.TE}.txt"
    output: "report.html"
    run:
        from snakemake.utils import report
        with open(input[1]) as stats:
            f=print (stats.read())
        report("""
        Differentiate between endemic and non-endemic sequences based on read pairs workflow
        ====================================================================================
        
        Reads were converted from fastq to fasta and hard-masked if quality under 20. Reads were then blasted against TE        of interest with evalue cutoff of 1e-03. Reads with positive hits were filtered and repeatmasked for TE of inter        est to confirm blast result. Read mates of reads with confirmed TE were checked if they are also TE sequences. I        f not, they were masked for all TEs and then blasted against different taxonomy databases to assess taxon of ori        gin (original or other phylum). 
        {f}
        S_
        """, 
        output[0], R=input[0], S=input[1]
        )
